### English Summary

#### 1. **High-Level Problem**:
The paper *"Attention is All You Need"* introduces a new model architecture called the **Transformer**. Traditional sequence transduction models such as Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs) often struggle with long-range dependencies and computational efficiency due to their sequential nature. The authors propose a new approach that relies solely on **attention mechanisms** without using recurrence or convolutions, enabling more efficient training and improved performance in tasks such as machine translation.

#### 2. **Limitations of Previous Research**:
Previous models, especially RNNs (like LSTMs and GRUs), face several limitations:
   * **Sequential computation**: RNNs process inputs sequentially, which hinders **parallelization** and increases training time, especially for long sequences.
   * **Difficulty in learning long-range dependencies**: In RNNs, the path length for capturing relationships between distant input elements is long, making it harder to learn long-range dependencies.
   * **Convolutional models**: While CNN-based models improved efficiency, they still required multiple layers to capture distant dependencies, increasing complexity.

#### 3. **Motivation for the Proposed Method**:
The Transformer was motivated by the need for a model that can efficiently capture long-range dependencies while being highly parallelizable. The authors designed the model to focus purely on **self-attention** mechanisms, which allow for constant path lengths between dependencies, regardless of their distance in the input sequence. This enables efficient parallelization, reducing training time while maintaining or improving translation quality.

#### 4. **Methodology**:
The Transformer architecture follows the standard **encoder-decoder structure** but with significant innovations:
* **Encoder and Decoder**: Both the encoder and decoder are composed of multiple identical layers. Each encoder layer consists of:
  - **Multi-head self-attention**: This allows the model to focus on different parts of the input sentence simultaneously.
  - **Position-wise feed-forward network**: A fully connected network applied to each position independently.
  Residual connections and **layer normalization** are used to stabilize training.

* **Multi-head Attention Mechanism**: The core of the Transformer is the **multi-head attention** mechanism. It allows the model to jointly attend to different parts of the input sequence by computing attention multiple times in parallel (each with a different "head"). The results are then concatenated and processed together.
  - **Scaled Dot-Product Attention**: The attention mechanism works by calculating the dot product of queries and keys, scaling the result, and applying a softmax function to compute attention weights. The weighted sum of the values is then taken as the output.

* **Positional Encoding**: Since the Transformer doesn't have recurrence, it uses **positional encodings** to inject information about the order of the input tokens. This is done using sine and cosine functions of different frequencies.

* **Input-Feeding**: In the decoder, attention is applied to both the encoder's outputs and the previous decoder outputs (masked to prevent peeking at future tokens).

#### 5. **Contribution of the Method**:
The Transformer introduces several key contributions:
* **Completely removes recurrence and convolution**: The model is based entirely on attention mechanisms, making it highly parallelizable and reducing training time significantly.
* **State-of-the-art results in translation**: The model outperforms previous models on both the **WMT 2014 English-to-German** and **English-to-French** translation tasks, setting new records with BLEU scores of **28.4** and **41.0**, respectively.
* **Improved scalability**: The self-attention mechanism ensures that the model scales efficiently with input length and can be trained faster than RNN-based models while maintaining high translation quality.

---

### Korean Summary

#### 1. **해결하고자 하는 문제**:
논문 *"Attention is All You Need"*에서는 **Transformer**라는 새로운 모델 구조를 소개합니다. 기존의 순환 신경망(RNN)과 합성곱 신경망(CNN)을 기반으로 한 시퀀스 변환 모델은 긴 의존성을 학습하거나 계산 효율성 면에서 문제가 있었습니다. 저자들은 순환이나 합성곱 없이 **주의 메커니즘(Attention)**만으로 작동하는 모델을 제안하여, 더 효율적인 학습과 성능 향상을 목표로 했습니다.

#### 2. **이전 연구의 한계**:
기존 모델, 특히 RNN(LSTM, GRU)은 여러 한계를 가지고 있었습니다:
   * **순차적 계산**: RNN은 입력을 순차적으로 처리해야 하므로 **병렬화가 어렵고** 긴 시퀀스를 처리할 때 시간이 오래 걸립니다.
   * **장거리 의존성 학습의 어려움**: RNN에서는 멀리 떨어진 입력 간의 관계를 학습하는 데 시간이 많이 걸리며, 학습하기 어려운 경향이 있습니다.
   * **합성곱 모델의 한계**: CNN 기반 모델은 효율성을 높였으나, 여전히 멀리 떨어진 의존성을 학습하려면 여러 층을 쌓아야 하므로 복잡성이 증가했습니다.

#### 3. **제안된 방법의 동기**:
Transformer 모델은 **장거리 의존성**을 효율적으로 학습하면서 **병렬화**가 가능한 모델을 만들기 위한 동기에서 출발했습니다. 저자들은 **자기 주의(Self-attention)** 메커니즘을 통해 입력 시퀀스 내의 모든 단어 간 의존성을 짧은 경로로 연결하고, 이를 통해 **훈련 시간을 단축**하면서도 번역 품질을 유지하거나 개선할 수 있는 모델을 설계했습니다.

#### 4. **방법론**:
Transformer 구조는 **인코더-디코더 구조**를 따르며, 다음과 같은 혁신을 포함합니다:
* **인코더와 디코더**: 인코더와 디코더는 여러 개의 동일한 층으로 구성됩니다. 각 인코더 층은 다음을 포함합니다:
  - **다중 헤드 자기 주의(Multi-head self-attention)**: 입력 문장의 여러 부분에 동시에 집중할 수 있습니다.
  - **위치별 피드 포워드 네트워크**: 각 위치에서 독립적으로 적용되는 완전 연결 신경망입니다. 잔차 연결과 **레이어 정규화**가 학습을 안정화합니다.

* **다중 헤드 주의 메커니즘**: Transformer의 핵심은 **다중 헤드 주의(Multi-head attention)** 메커니즘입니다. 입력 시퀀스의 여러 부분에 여러 번 주의를 기울이고, 그 결과를 결합하여 처리합니다.
  - **스케일된 내적 주의(Scaled Dot-Product Attention)**: 이 메커니즘은 쿼리와 키 간의 내적을 계산하고 결과를 스케일링한 뒤 소프트맥스 함수를 적용하여 주의 가중치를 계산합니다. 그런 다음 값을 가중 합하여 결과를 생성합니다.

* **위치 인코딩(Positional Encoding)**: Transformer는 순환 구조가 없기 때문에 입력 토큰의 순서를 모델에 제공하기 위해 **위치 인코딩**을 사용합니다. 이는 서로 다른 주파수의 사인 및 코사인 함수로 수행됩니다.

* **입력 피드백**: 디코더에서는 인코더의 출력과 이전 디코더 출력을 기반으로 주의를 적용하여 출력 간 일관성을 유지합니다.

#### 5. **기여 내용**:
Transformer는 다음과 같은 주요 기여를 합니다:
* **순환 및 합성곱을 완전히 제거**: 모델은 주의 메커니즘만을 사용하여 높은 병렬 처리 능력을 제공하며, 훈련 시간을 크게 단축합니다.
* **번역 작업에서 최첨단 성능 달성**: 모델은 **WMT 2014 영어-독일어** 및 **영어-프랑스어** 번역 작업에서 새로운 BLEU 점수 기록을 세우며, 각각 **28.4**와 **41.0**을 기록했습니다.
* **확장성 향상**: 자기 주의 메커니즘은 입력 길이에 관계없이 모델이 효율적으로 확장될 수 있도록 하며, RNN 기반 모델보다 빠르게 훈련할 수 있습니다.

--- 

This paper introduces the **Transformer** model, a revolutionary architecture in deep learning, particularly in the realm of natural language processing (NLP).
